---
title: "homework 4"
author: "Ricky Sun"
date: "2023-02-26"
output: 
  html_document: default
  github_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Goal: Understand and implement various ways to approximate test error.

In the ISLR book, read section 6.1.3 “Choosing the Optimal Model” and section 5.1 “Cross-Validation”. Extend and convert the attached effective-df-aic-bic-mcycle.R R script into an R markdown file that accomplishes the following tasks.

1. Randomly split the mcycle data into training (75%) and validation (25%) subsets.

2. Using the mcycle data, consider predicting the mean acceleration as a function of time. Use the Nadaraya-Watson method with the k-NN kernel function to create a series of prediction models by varying the tuning parameter over a sequence of values. (hint: the script already implements this)

3. With the squared-error loss function, compute and plot the training error, AIC, BIC, and validation error (using the validation data) as functions of the tuning parameter.

4. For each value of the tuning parameter, Perform 5-fold cross-validation using the combined training and validation data. This results in 5 estimates of test error per tuning parameter value.

5. Plot the CV-estimated test error (average of the five estimates from each fold) as a function of the tuning parameter. Add vertical line segments to the figure (using the segments function in R) that represent one “standard error” of the CV-estimated test error (standard deviation of the five estimates from each fold).

6. Interpret the resulting figures and select a suitable value for the tuning parameter.

--------------------------------------------------------------------------------

# Load libraries and data
```{r}
library('MASS') ## for 'mcycle'
library('manipulate') ## for 'manipulate'
```

```{r}
y <- mcycle$accel
x <- matrix(mcycle$times, length(mcycle$times), 1)

plot(x, y, xlab="Time (ms)", ylab="Acceleration (g)")
```

# Define functions
```{r}
## Helper function to view kernel (smoother) matrix
matrix_image <- function(x) {
  rot <- function(x) t(apply(x, 2, rev))
  cls <- rev(gray.colors(20, end=1))
  image(rot(x), col=cls, axes=FALSE)
  xlb <- pretty(1:ncol(x))
  xat <- (xlb-0.5)/ncol(x)
  ylb <- pretty(1:nrow(x))
  yat <- (ylb-0.5)/nrow(x)
  axis(3, at=xat, labels=xlb)
  axis(2, at=yat, labels=ylb)
  mtext('Rows', 2, 3)
  mtext('Columns', 3, 3)
}

## Compute effective df using NW method
## y  - n x 1 vector of training outputs
## x  - n x p matrix of training inputs
## kern  - kernel function to use
## ... - arguments to pass to kernel function
effective_df <- function(y, x, kern, ...) {
  y_hat <- nadaraya_watson(y, x, x,
    kern=kern, ...)
  sum(diag(attr(y_hat, 'k')))
}

## loss function
## y    - train/test y
## yhat - predictions at train/test x
loss_squared_error <- function(y, yhat)
  (y - yhat)^2

## test/train error
## y    - train/test y
## yhat - predictions at train/test x
## loss - loss function
error <- function(y, yhat, loss=loss_squared_error)
  mean(loss(y, yhat))

## AIC
## y    - training y
## yhat - predictions at training x
## d    - effective degrees of freedom
aic <- function(y, yhat, d)
  error(y, yhat) + 2/length(y)*d

## BIC
## y    - training y
## yhat - predictions at training x
## d    - effective degrees of freedom
bic <- function(y, yhat, d)
  error(y, yhat) + log(length(y))/length(y)*d
```

```{r}
## Epanechnikov kernel function
## x  - n x p matrix of training inputs
## x0 - 1 x p input where to make prediction
## lambda - bandwidth (neighborhood size)
kernel_epanechnikov <- function(x, x0, lambda=1) {
  d <- function(t)
    ifelse(t <= 1, 3/4*(1-t^2), 0)
  z <- t(t(x) - x0)
  d(sqrt(rowSums(z*z))/lambda)
}

## k-NN kernel function
## x  - n x p matrix of training inputs
## x0 - 1 x p input where to make prediction
## k  - number of nearest neighbors
kernel_k_nearest_neighbors <- function(x, x0, k=1) {
  ## compute distance betwen each x and x0
  z <- t(t(x) - x0)
  d <- sqrt(rowSums(z*z))

  ## initialize kernel weights to zero
  w <- rep(0, length(d))
  
  ## set weight to 1 for k nearest neighbors
  w[order(d)[1:k]] <- 1
  
  return(w)
}

## Make predictions using the NW method
## y  - n x 1 vector of training outputs
## x  - n x p matrix of training inputs
## x0 - m x p matrix where to make predictions
## kern  - kernel function to use
## ... - arguments to pass to kernel function
nadaraya_watson <- function(y, x, x0, kern, ...) {
  k <- t(apply(x0, 1, function(x0_) {
    k_ <- kern(x, x0_, ...)
    k_/sum(k_)
  }))
  yhat <- drop(k %*% y)
  attr(yhat, 'k') <- k
  return(yhat)
}
```

# Make predictions
```{r}
## make predictions using NW method at training inputs
y_hat <- nadaraya_watson(y, x, x,
  kernel_epanechnikov, lambda=5)

## view kernel (smoother) matrix
matrix_image(attr(y_hat, 'k'))

## compute effective degrees of freedom
edf <- effective_df(y, x, kernel_epanechnikov, lambda=5)
aic(y, y_hat, edf)
bic(y, y_hat, edf)

## create a grid of inputs 
x_plot <- matrix(seq(min(x),max(x),length.out=100),100,1)

## make predictions using NW method at each of grid points
y_hat_plot <- nadaraya_watson(y, x, x_plot,
  kernel_epanechnikov, lambda=1)

## plot predictions
plot(x, y, xlab="Time (ms)", ylab="Acceleration (g)")
lines(x_plot, y_hat_plot, col="#882255", lwd=2) 
```

```{r}
## how does k affect shape of predictor and eff. df using k-nn kernel ?
# manipulate({
#   ## make predictions using NW method at training inputs
#   y_hat <- nadaraya_watson(y, x, x,
#     kern=kernel_k_nearest_neighbors, k=k_slider)
#   edf <- effective_df(y, x, 
#     kern=kernel_k_nearest_neighbors, k=k_slider)
#   aic_ <- aic(y, y_hat, edf)
#   bic_ <- bic(y, y_hat, edf)
#   y_hat_plot <- nadaraya_watson(y, x, x_plot,
#     kern=kernel_k_nearest_neighbors, k=k_slider)
#   plot(x, y, xlab="Time (ms)", ylab="Acceleration (g)")
#   legend('topright', legend = c(
#     paste0('eff. df = ', round(edf,1)),
#     paste0('aic = ', round(aic_, 1)),
#     paste0('bic = ', round(bic_, 1))),
#     bty='n')
#   lines(x_plot, y_hat_plot, col="#882255", lwd=2) 
# }, k_slider=slider(1, 10, initial=3, step=1))
```


# Homework part
1. Randomly split the mcycle data into training (75%) and validation (25%) subsets.
```{r}
set.seed(123) # set seed for reproducibility

n <- nrow(mcycle)
train_idx <- sample(n, floor(0.75*n), replace = FALSE)
train <- mcycle[train_idx, ]
test <- mcycle[-train_idx, ]

x_train <- matrix(train$times, length(train$times), 1)
x_test <- matrix(test$times, length(test$times), 1)
y_train <- train$accel
y_test <- test$accel
```

2. Using the mcycle data, consider predicting the mean acceleration as a function of time. Use the Nadaraya-Watson method with the k-NN kernel function to create a series of prediction models by varying the tuning parameter over a sequence of values. (hint: the script already implements this)
```{r warning = FALSE}
k_seq <- seq(1, 20, by = 1)

models <- lapply(k_seq, function(k) {

  y_hat_train <- nadaraya_watson(y_train, x_train, x_train, kern = kernel_k_nearest_neighbors, k = k)
  y_hat_test <- nadaraya_watson(y_train, x_train, x_test, kern = kernel_k_nearest_neighbors, k = k)
  
  list(y_hat_train = y_hat_train, y_hat_test = y_hat_test)
})


mse_vec <- numeric(length(models))
for (i in seq_along(models)) {
  mse_vec[i] <- error(y_test, models[[i]]$y_hat_test)
}

print(data.frame(k = k_seq, MSE = mse_vec))
```

3. With the squared-error loss function, compute and plot the training error, AIC, BIC, and validation error (using the validation data) as functions of the tuning parameter.

```{r warning = FALSE}
k_seq <- seq(1, 20, by = 1)

train_error <- rep(0, length(k_seq))
test_error <- rep(0, length(k_seq))
edf <- rep(0, length(k_seq))
aic_list <- rep(0, length(k_seq))
bic_list <- rep(0, length(k_seq))

for (k in k_seq) {
  y_hat_train <- nadaraya_watson(y_train, x_train, x_train, kern=kernel_k_nearest_neighbors, k=k)
  train_error[k] <- error(y_train, y_hat_train)
  
  edf[k] <- effective_df(y_train, x_train, kern=kernel_k_nearest_neighbors, k=k)
  aic_list[k] <- aic(y_train, y_hat_train, edf[k])
  bic_list[k] <- bic(y_train, y_hat_train, edf[k])
  
  y_hat_test <- nadaraya_watson(y_train, x_train, x_test, kern=kernel_k_nearest_neighbors, k=k)
  test_error[k] <- error(y_test, y_hat_test)
}

df <- data.frame(k = k_seq, train_error=train_error, edf=edf, aic_list=aic_list, bic_list=bic_list, test_error=test_error)

```


```{r}
# library(ggplot2)

# plot training error
plot(df$k, df$train_error, type = "l", xlab = "k", ylab = "Error", col = "orange")
# plot validation error
lines(df$k, df$test_error, col = "blue")
# add legend
legend("top", legend = c("Training Error", "Validation Error"), 
       col = c("orange", "blue"), lty = 1)

# plot AIC
plot(df$k, df$aic_list, type = "l", xlab = "k", ylab = "Error", col = "purple")
# plot BIC
lines(df$k, df$bic_list, col = "green")
# add legend
legend("top", legend = c("AIC", "BIC"), 
       col = c("purple", "green"), lty = 1)

plot(df$k, df$aic_list, type = "l", xlab = "k", ylab = "Error", col = "purple")
legend("top", legend = c("AIC"), 
       col = c("purple"), lty = 1)
plot(df$k, df$bic_list, type = "l", xlab = "k", ylab = "Error", col = "green")
legend("top", legend = c("BIC"), 
       col = c("green"), lty = 1)

# ggplot(df, aes(x = k)) +
#   geom_line(aes(y = train_error), color = "blue", size = 1) +
#   geom_line(aes(y = aic_list), color = "red", size = 1) +
#   geom_line(aes(y = bic_list), color = "green", size = 1) +
#   geom_line(aes(y = test_error), color = "orange", size = 1) +
#   scale_x_continuous(breaks = k_seq) +
#   labs(x = "Tuning Parameter (k)",
#        y = "Error",
#        title = "Nadaraya-Watson with k-NN Kernel") +
#   theme(plot.title = element_text(hjust = 0.5, size = 16),
#         axis.text = element_text(size = 12),
#         axis.title = element_text(size = 14))
```

4. For each value of the tuning parameter, Perform 5-fold cross-validation using the combined training and validation data. This results in 5 estimates of test error per tuning parameter value.
```{r}
library(caret)

## 5-fold cross-validation of knnreg model
## create five folds
set.seed(123)
mcycle_flds  <- createFolds(mcycle$accel, k=5)
print(mcycle_flds)
sapply(mcycle_flds, length)  ## not all the same length

cvknnreg <- function(kNN = 10, flds=mcycle_flds) {
  cverr <- rep(NA, length(flds))
  for(tst_idx in 1:length(flds)) { ## for each fold
    
    ## get training and testing data
    mycycle_trn <- mcycle[-flds[[tst_idx]],]
    mycycle_tst <- mcycle[ flds[[tst_idx]],]
    
    ## fit kNN model to training data
    knn_fit <- knnreg(accel ~ times,
                      k=kNN, data=mycycle_trn)
    
    ## compute test error on testing data
    pre_tst <- predict(knn_fit, mycycle_tst)
    cverr[tst_idx] <- mean((mycycle_tst$accel - pre_tst)^2)
  }
  return(cverr)
}

## Compute 5-fold CV for kNN = 1:20
cverrs <- sapply(1:20, cvknnreg)
print(cverrs) ## rows are k-folds (1:5), cols are kNN (1:20)
cverrs_mean <- apply(cverrs, 2, mean)
cverrs_sd   <- apply(cverrs, 2, sd)
```

5. Plot the CV-estimated test error (average of the five estimates from each fold) as a function of the tuning parameter. Add vertical line segments to the figure (using the segments function in R) that represent one “standard error” of the CV-estimated test error (standard deviation of the five estimates from each fold).
```{r}
## Plot the results of 5-fold CV for kNN | k = 1:20
plot(x=1:20, y=cverrs_mean, 
     ylim=range(cverrs),
     xlab="'k' in kNN", ylab="CV Estimate of Test Error")
segments(x0=1:20, x1=1:20,
         y0=cverrs_mean-cverrs_sd,
         y1=cverrs_mean+cverrs_sd)
best_idx <- which.min(cverrs_mean)
points(x=best_idx, y=cverrs_mean[best_idx], pch=20)
abline(h=cverrs_mean[best_idx] + cverrs_sd[best_idx], lty=3)
```


6. Interpret the resulting figures and select a suitable value for the tuning parameter.
It seems that k = 10 is the best value in this case for the tuning parameter; k = 20 is the simpliest model and k = 3 is the most complicated model that satisfy the one standard deviation rule, 



